{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "976a0ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weight: runs/train/exp130/weights/train-200.pt\n",
      "torch.Size([1, 105, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taehyun/workspace/childtoy/MotionReasoning/MoCAM/model/model.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import yaml\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_proc.emotionmocap_dataset import EmotionDataset\n",
    "from data_proc.utils import increment_path\n",
    "from model.model import TCN, PURE1D\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "from CAM.eigen_cam import EigenCAM\n",
    "\n",
    "from CAM.guided_backprop import GuidedBackpropReLUModel\n",
    "from CAM.utils.image import show_cam_on_image, deprocess_image, preprocess_image\n",
    "from CAM.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "parser = argparse.ArgumentParser()\n",
    "project='runs/train'\n",
    "weight='latest'\n",
    "exp_name='exp130'\n",
    "data_path='/home/taehyun/workspace/childtoy/MotionReasoning/dataset/mocap_emotion_rig'\n",
    "window=80\n",
    "batch_size=1\n",
    "\n",
    "processed_data_dir='processed_data_mocam/'\n",
    "\n",
    "save_dir = Path(os.path.join('runs', 'train', exp_name))\n",
    "wdir = save_dir / 'weights'\n",
    "weights = os.listdir(wdir)\n",
    "\n",
    "if weight == 'latest':\n",
    "    weights_paths = [wdir / weight for weight in weights]\n",
    "    weight_path = max(weights_paths , key = os.path.getctime)\n",
    "else:\n",
    "    weight_path = wdir / ('train-' + weight + '.pt')\n",
    "ckpt = torch.load(weight_path, map_location=device)\n",
    "print(f\"Loaded weight: {weight_path}\")\n",
    "\n",
    "\n",
    "# Load LAFAN Dataset\n",
    "Path(processed_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "emotion_dataset = EmotionDataset(data_dir=data_path, processed_data_dir=processed_data_dir, train=False, device=device, window=window)\n",
    "emotion_data_loader = DataLoader(emotion_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "n_hid = 70\n",
    "n_level = 4\n",
    "channel_sizes = [n_hid] * n_level\n",
    "kernel_size = 5\n",
    "model = PURE1D(input_channels, n_classes, kernel_size=kernel_size, dropout=0)\n",
    "model.load_state_dict(ckpt['TCN'])\n",
    "model.eval()\n",
    "correct = 0\n",
    "n_classes = ckpt['n_classes']\n",
    "input_channels = ckpt['input_channels']\n",
    "seq_length = 40\n",
    "n_hid = ckpt['n_hid']\n",
    "n_level = ckpt['n_level']\n",
    "n_classes = 7\n",
    "input_channels = 105\n",
    "origin_data = iter(emotion_data_loader).next()# confusion_matrix = torch.zeros(7, 7)\n",
    "local_q = origin_data[\"local_q\"].to(device)\n",
    "q_vel = origin_data[\"q_vel\"].to(device) \n",
    "q_acc = origin_data[\"q_acc\"].to(device) \n",
    "labels = origin_data[\"labels\"].to(device)\n",
    "data = torch.cat([local_q, q_vel, q_acc], axis=2)\n",
    "data = data.permute(0,2,1)\n",
    "output = model(data)\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcb1ae74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 105, 80])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "303b53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = EigenCAM(model=model, target_layers = model.net.net3, use_cuda=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c101b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EigenCAM' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4ef783462258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'EigenCAM' object has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "with cam :\n",
    "\n",
    "    # AblationCAM and ScoreCAM have batched implementations.\n",
    "    # You can override the internal batch size for faster computation.\n",
    "    cam.batch_size = 1\n",
    "    grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                        targets=targets,\n",
    "                        aug_smooth=args.aug_smooth,\n",
    "                        eigen_smooth=args.eigen_smooth)\n",
    "\n",
    "    # Here grayscale_cam has only one image in the batch\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    # cam_image is RGB encoded whereas \"cv2.imwrite\" requires BGR encoding.\n",
    "    cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21346d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from CAM.utils.find_layer import replace_all_layer_type_recursive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f8cc48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def slerp(x, y, a):\n",
    "    \"\"\"\n",
    "    Perfroms spherical linear interpolation (SLERP) between x and y, with proportion a\n",
    "\n",
    "    :param x: quaternion tensor\n",
    "    :param y: quaternion tensor\n",
    "    :param a: indicator (between 0 and 1) of completion of the interpolation.\n",
    "    :return: tensor of interpolation results\n",
    "    \"\"\"\n",
    "    device = x.device\n",
    "    len = torch.sum(x * y, dim=-1)\n",
    "\n",
    "    neg = len < 0.0\n",
    "    len[neg] = -len[neg]\n",
    "    y[neg] = -y[neg]\n",
    "\n",
    "    a = torch.zeros_like(x[..., 0]) + a\n",
    "    amount0 = torch.zeros(a.shape, device=device)\n",
    "    amount1 = torch.zeros(a.shape, device=device)\n",
    "\n",
    "    linear = (1.0 - len) < 0.01\n",
    "    omegas = torch.arccos(len[~linear])\n",
    "    sinoms = torch.sin(omegas)\n",
    "\n",
    "    amount0[linear] = 1.0 - a[linear]\n",
    "    amount0[~linear] = torch.sin((1.0 - a[~linear]) * omegas) / sinoms\n",
    "\n",
    "    amount1[linear] = a[linear]\n",
    "    amount1[~linear] = torch.sin(a[~linear] * omegas) / sinoms\n",
    "    # res = amount0[..., np.newaxis] * x + amount1[..., np.newaxis] * y\n",
    "\n",
    "    res = amount0.unsqueeze(2) * x + amount1.unsqueeze(2) * y\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "614e1a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3482, -0.6039,  0.3385, -0.5345,  1.0330,  0.3379,  0.7086,\n",
       "          -1.2253, -0.4736, -0.1716,  0.5938, -0.5642,  0.0935, -0.0975,\n",
       "           1.0120,  0.2344,  0.1243, -0.3263, -0.1054,  0.3421,  0.1136,\n",
       "          -0.5162,  0.0650, -0.1728, -0.3562, -0.2133,  0.3854, -0.1702,\n",
       "          -0.6525,  0.2746,  0.2509,  0.4100, -0.0539, -0.1555, -0.0955]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "slerp(minibatch_pose_input[:,0:1], minibatch_pose_input[:,1:2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cd05e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3208, -0.6984,  0.1608, -0.6784,  0.8908,  0.4103,  0.6880,\n",
       "          -1.2213, -0.5218,  0.1570,  0.4645, -0.5792,  0.1383,  0.0084,\n",
       "           1.0896,  0.0180,  0.1263, -0.3861, -0.0862,  0.3160,  0.1249,\n",
       "          -0.5277,  0.0879, -0.1222, -0.4131, -0.1709,  0.3516,  0.0086,\n",
       "          -0.6398,  0.2707,  0.3412, -0.2118,  0.0266, -0.2267, -0.0706]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_pose_input[:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c81ec7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3753, -0.7690,  0.4045, -0.3956,  1.0158,  0.3112,  0.6439,\n",
       "          -1.2066, -0.4249, -0.2364,  0.5893, -0.5288,  0.0688,  0.0719,\n",
       "           0.9805,  0.0739,  0.1063, -0.2881, -0.0945,  0.3488,  0.3066,\n",
       "          -0.5396,  0.1073,  0.4202, -0.9112,  0.0515,  0.3812,  1.7749,\n",
       "          -0.6355,  0.3403,  1.3667, -6.6347, -0.0482, -0.1574, -0.0926]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(minibatch_pose_input[:,0:1] - minibatch_pose_input[:,1:2])/2  + minibatch_pose_input[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_start_frame = 0 \n",
    "seq_len = int(local_q.size(1))\n",
    "expand_rate = int(teacher_len/student_len)\n",
    "for i in range(student_len):\n",
    "    interpolated = torcch\n",
    "for i in range(expand_rate):\n",
    "        dt = 1 / expand_rate\n",
    "        interpolated[:,i:i+1,:] = slerp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4bff0a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 35])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "teacher_len = 60\n",
    "num_joint = 35\n",
    "torch.zeros([batch_size, teacher_len, num_joint]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c7e3ccb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.33898305,  0.6779661 ,  1.01694915,  1.3559322 ,\n",
       "        1.69491525,  2.03389831,  2.37288136,  2.71186441,  3.05084746,\n",
       "        3.38983051,  3.72881356,  4.06779661,  4.40677966,  4.74576271,\n",
       "        5.08474576,  5.42372881,  5.76271186,  6.10169492,  6.44067797,\n",
       "        6.77966102,  7.11864407,  7.45762712,  7.79661017,  8.13559322,\n",
       "        8.47457627,  8.81355932,  9.15254237,  9.49152542,  9.83050847,\n",
       "       10.16949153, 10.50847458, 10.84745763, 11.18644068, 11.52542373,\n",
       "       11.86440678, 12.20338983, 12.54237288, 12.88135593, 13.22033898,\n",
       "       13.55932203, 13.89830508, 14.23728814, 14.57627119, 14.91525424,\n",
       "       15.25423729, 15.59322034, 15.93220339, 16.27118644, 16.61016949,\n",
       "       16.94915254, 17.28813559, 17.62711864, 17.96610169, 18.30508475,\n",
       "       18.6440678 , 18.98305085, 19.3220339 , 19.66101695, 20.        ])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "75acb53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 36 25 19  4 24 30 32]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.randint(0, teacher_len - student_len,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bd2351b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expand_rate 3\n"
     ]
    }
   ],
   "source": [
    "mask_start_frame = 0 \n",
    "batch_size = 1\n",
    "teacher_len = 60\n",
    "student_len = 20\n",
    "num_joint = 35\n",
    "np.random.randint(0, teacher_len - student_len)\n",
    "local_start_idx = np.random.randint(0, teacher_len - student_len,8)\n",
    "global_start_idx = [10, 20]\n",
    "seq_len = int(local_q.size(1))\n",
    "expand_rate = int(teacher_len/student_len)\n",
    "print('expand_rate', expand_rate)\n",
    "interpolated = torch.zeros([batch_size, teacher_len, num_joint])\n",
    "\n",
    "data_seq = []\n",
    "data_seq.append(local_q[:,global_start_idx[0]:global_start_idx[0]  + teacher_len])\n",
    "data_seq.append(local_q[:,global_start_idx[1]:global_start_idx[1]  + teacher_len])\n",
    "\n",
    "local_dt_list = np.linspace(0, student_len-1, teacher_len)\n",
    "for k in range(8):\n",
    "    start_idx = local_start_idx[k]\n",
    "    minibatch_pose_input = local_q[:,start_idx:start_idx + student_len]\n",
    "    j = 0 \n",
    "    for i in range(teacher_len-1):\n",
    "        interpolate_start = minibatch_pose_input[:,int(dt_list[i])].unsqueeze(1)\n",
    "        interpolate_end = minibatch_pose_input[:,int(dt_list[i])+1].unsqueeze(1)\n",
    "        interpolated[:,j] = slerp(interpolate_start, interpolate_end, dt_list[i]-int(dt_list[i]))\n",
    "        j+=1\n",
    "    interpolated[:,-1] = minibatch_pose_input[:,-1]\n",
    "    data_seq.append(interpolated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c249c695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vits.__dict__[args.arch](patch_size=args.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dfcdce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dino/archive/main.zip\" to /home/taehyun/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /home/taehyun/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d14d6fb8c5e43659515796e7555fba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/82.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b82d8e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vits16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845dd4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69e330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "497ed903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expand_rate 3\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 0\n",
      "0.0\n",
      "start 0\n",
      "end 0\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 0\n",
      "0.3333333333333333\n",
      "start 0\n",
      "end 1\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 0\n",
      "0.6666666666666666\n",
      "start 0\n",
      "end 2\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 1\n",
      "0.0\n",
      "start 3\n",
      "end 3\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 1\n",
      "0.3333333333333333\n",
      "start 3\n",
      "end 4\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 1\n",
      "0.6666666666666666\n",
      "start 3\n",
      "end 5\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 2\n",
      "0.0\n",
      "start 6\n",
      "end 6\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 2\n",
      "0.3333333333333333\n",
      "start 6\n",
      "end 7\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 2\n",
      "0.6666666666666666\n",
      "start 6\n",
      "end 8\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 3\n",
      "0.0\n",
      "start 9\n",
      "end 9\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 3\n",
      "0.3333333333333333\n",
      "start 9\n",
      "end 10\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 3\n",
      "0.6666666666666666\n",
      "start 9\n",
      "end 11\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 4\n",
      "0.0\n",
      "start 12\n",
      "end 12\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 4\n",
      "0.3333333333333333\n",
      "start 12\n",
      "end 13\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 4\n",
      "0.6666666666666666\n",
      "start 12\n",
      "end 14\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 5\n",
      "0.0\n",
      "start 15\n",
      "end 15\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 5\n",
      "0.3333333333333333\n",
      "start 15\n",
      "end 16\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 5\n",
      "0.6666666666666666\n",
      "start 15\n",
      "end 17\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 6\n",
      "0.0\n",
      "start 18\n",
      "end 18\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 6\n",
      "0.3333333333333333\n",
      "start 18\n",
      "end 19\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 6\n",
      "0.6666666666666666\n",
      "start 18\n",
      "end 20\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 7\n",
      "0.0\n",
      "start 21\n",
      "end 21\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 7\n",
      "0.3333333333333333\n",
      "start 21\n",
      "end 22\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 7\n",
      "0.6666666666666666\n",
      "start 21\n",
      "end 23\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 8\n",
      "0.0\n",
      "start 24\n",
      "end 24\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 8\n",
      "0.3333333333333333\n",
      "start 24\n",
      "end 25\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 8\n",
      "0.6666666666666666\n",
      "start 24\n",
      "end 26\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 9\n",
      "0.0\n",
      "start 27\n",
      "end 27\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 9\n",
      "0.3333333333333333\n",
      "start 27\n",
      "end 28\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 9\n",
      "0.6666666666666666\n",
      "start 27\n",
      "end 29\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 10\n",
      "0.0\n",
      "start 30\n",
      "end 30\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 10\n",
      "0.3333333333333333\n",
      "start 30\n",
      "end 31\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 10\n",
      "0.6666666666666666\n",
      "start 30\n",
      "end 32\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 11\n",
      "0.0\n",
      "start 33\n",
      "end 33\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 11\n",
      "0.3333333333333333\n",
      "start 33\n",
      "end 34\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 11\n",
      "0.6666666666666666\n",
      "start 33\n",
      "end 35\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 12\n",
      "0.0\n",
      "start 36\n",
      "end 36\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 12\n",
      "0.3333333333333333\n",
      "start 36\n",
      "end 37\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 12\n",
      "0.6666666666666666\n",
      "start 36\n",
      "end 38\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 13\n",
      "0.0\n",
      "start 39\n",
      "end 39\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 13\n",
      "0.3333333333333333\n",
      "start 39\n",
      "end 40\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 13\n",
      "0.6666666666666666\n",
      "start 39\n",
      "end 41\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 14\n",
      "0.0\n",
      "start 42\n",
      "end 42\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 14\n",
      "0.3333333333333333\n",
      "start 42\n",
      "end 43\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 14\n",
      "0.6666666666666666\n",
      "start 42\n",
      "end 44\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 15\n",
      "0.0\n",
      "start 45\n",
      "end 45\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 15\n",
      "0.3333333333333333\n",
      "start 45\n",
      "end 46\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 15\n",
      "0.6666666666666666\n",
      "start 45\n",
      "end 47\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 16\n",
      "0.0\n",
      "start 48\n",
      "end 48\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 16\n",
      "0.3333333333333333\n",
      "start 48\n",
      "end 49\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 16\n",
      "0.6666666666666666\n",
      "start 48\n",
      "end 50\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 17\n",
      "0.0\n",
      "start 51\n",
      "end 51\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 17\n",
      "0.3333333333333333\n",
      "start 51\n",
      "end 52\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 17\n",
      "0.6666666666666666\n",
      "start 51\n",
      "end 53\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 18\n",
      "0.0\n",
      "start 54\n",
      "end 54\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 18\n",
      "0.3333333333333333\n",
      "start 54\n",
      "end 55\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "i 18\n",
      "0.6666666666666666\n",
      "start 54\n",
      "end 56\n"
     ]
    }
   ],
   "source": [
    "mask_start_frame = 0 \n",
    "batch_size = 1\n",
    "teacher_len = 60\n",
    "student_len = 20\n",
    "num_joint = 35\n",
    "seq_len = int(local_q.size(1))\n",
    "expand_rate = int(teacher_len/student_len)\n",
    "print('expand_rate', expand_rate)\n",
    "interpolated = torch.zeros([batch_size, teacher_len, num_joint])\n",
    "minibatch_pose_input = local_q[:,0:student_len]\n",
    "for i in range(student_len-1):\n",
    "    interpolate_start = minibatch_pose_input[:,i].unsqueeze(1)\n",
    "    interpolate_end = minibatch_pose_input[:,i+1].unsqueeze(1)\n",
    "    for j in range(expand_rate):\n",
    "        if i == 0 :\n",
    "            dt = 1 / (expand_rate)\n",
    "            interpolated[:,expand_rate*(i-1):expand_rate*(i-1)+j] = slerp(interpolate_start, interpolate_end, dt*j)\n",
    "        else : \n",
    "            dt = 1 / (expand_rate)\n",
    "            interpolated[:,expand_rate*(i-1):expand_rate*(i-1)+j] = slerp(interpolate_start, interpolate_end, dt*j)\n",
    "# print(intte)    \n",
    "# print('seq_len', seq_len)\n",
    "# minibatch_pose_input = local_q.reshape(local_q.size(0), seq_len, -1)\n",
    "# interpolated = torch.zeros_like(minibatch_pose_input)\n",
    "# for i in range(target_seq_len):\n",
    "#     interpolate_start = minibatch_pose_input[:,0:1]\n",
    "#     interpolate_end = minibatch_pose_input[:,seq_len-1:]\n",
    "#     for i in range(seq_len):\n",
    "#         dt = 1 / (seq_len-1)\n",
    "#         interpolated[:,i:i+1,:] = slerp(interpolate_start, interpolate_end, dt * i)\n",
    "\n",
    "#     assert torch.allclose(interpolated[:,0:1], interpolate_start)\n",
    "#     assert torch.allclose(interpolated[:,seq_len-1:], interpolate_end)\n",
    "# else:\n",
    "#     interpolate_start1 = minibatch_pose_input[:,0:1]\n",
    "#     interpolate_end1 = minibatch_pose_input[:,mask_start_frame:mask_start_frame+1]\n",
    "\n",
    "#     interpolate_start2 = minibatch_pose_input[:, mask_start_frame:mask_start_frame+1]\n",
    "#     interpolate_end2 = minibatch_pose_input[:,seq_len-1:]\n",
    "\n",
    "#     for i in range(mask_start_frame+1):\n",
    "#         dt = 1 / mask_start_frame\n",
    "#         interpolated[:,i:i+1,:] = slerp(interpolate_start1, interpolate_end1, dt * i)\n",
    "\n",
    "#     assert torch.allclose(interpolated[:,0:1], interpolate_start1)\n",
    "#     assert torch.allclose(interpolated[:,mask_start_frame:mask_start_frame+1], interpolate_end1)\n",
    "\n",
    "#     for i in range(mask_start_frame, seq_len):\n",
    "#         dt = 1 / (seq_len - mask_start_frame - 1)\n",
    "#         interpolated[:,i:i+1,:] = slerp(interpolate_start2, interpolate_end2, dt * (i - mask_start_frame))\n",
    "\n",
    "#     assert torch.allclose(interpolated[:,mask_start_frame:mask_start_frame+1], interpolate_start2)\n",
    "#     assert torch.allclose(interpolated[:,seq_len-1:], interpolate_end2)\n",
    "\n",
    "# interpolated = torch.nn.functional.normalize(interpolated, p=2.0, dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e37c26c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7014, -0.3513,  0.2116, -0.3492,  0.1766, -0.9525, -0.5637,  0.1699,\n",
       "         0.4342, -0.2971, -0.8326,  0.5925,  0.2770, -0.1130, -0.3524,  0.9334,\n",
       "         0.2243, -0.0978, -0.1088, -0.1055, -0.1362,  0.6335, -1.0102,  0.0710,\n",
       "         0.4224, -0.1058, -0.0804,  0.0507,  0.5528, -0.9270, -0.0478, -0.4164,\n",
       "        -0.1885, -0.0339,  0.1089])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated[0,-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96fd7bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 35])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56a2384a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 35])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc23aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dcbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model = models.resnet50(pretrained=True)\n",
    "target_layers = [model.layer4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6a592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae6010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--use-cuda', action='store_true', default=False,\n",
    "                        help='Use NVIDIA GPU acceleration')\n",
    "    parser.add_argument(\n",
    "        '--image-path',\n",
    "        type=str,\n",
    "        default='./examples/both.png',\n",
    "        help='Input image path')\n",
    "    parser.add_argument('--aug_smooth', action='store_true',\n",
    "                        help='Apply test time augmentation to smooth the CAM')\n",
    "    parser.add_argument(\n",
    "        '--eigen_smooth',\n",
    "        action='store_true',\n",
    "        help='Reduce noise by taking the first principle componenet'\n",
    "        'of cam_weights*activations')\n",
    "    parser.add_argument('--method', type=str, default='gradcam',\n",
    "                        choices=['gradcam', 'gradcam++',\n",
    "                                 'scorecam', 'xgradcam',\n",
    "                                 'ablationcam', 'eigencam',\n",
    "                                 'eigengradcam', 'layercam', 'fullgrad'],\n",
    "                        help='Can be gradcam/gradcam++/scorecam/xgradcam'\n",
    "                             '/ablationcam/eigencam/eigengradcam/layercam')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
    "    if args.use_cuda:\n",
    "        print('Using GPU for acceleration')\n",
    "    else:\n",
    "        print('Using CPU for computation')\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" python cam.py -image-path <path_to_image>\n",
    "    Example usage of loading an image, and computing:\n",
    "        1. CAM\n",
    "        2. Guided Back Propagation\n",
    "        3. Combining both\n",
    "    \"\"\"\n",
    "\n",
    "    args = get_args()\n",
    "    methods = \n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # Choose the target layer you want to compute the visualization for.\n",
    "    # Usually this will be the last convolutional layer in the model.\n",
    "    # Some common choices can be:\n",
    "    # Resnet18 and 50: model.layer4\n",
    "    # VGG, densenet161: model.features[-1]\n",
    "    # mnasnet1_0: model.layers[-1]\n",
    "    # You can print the model to help chose the layer\n",
    "    # You can pass a list with several target layers,\n",
    "    # in that case the CAMs will be computed per layer and then aggregated.\n",
    "    # You can also try selecting all layers of a certain type, with e.g:\n",
    "    # from pytorch_grad_cam.utils.find_layers import find_layer_types_recursive\n",
    "    # find_layer_types_recursive(model, [torch.nn.ReLU])\n",
    "    target_layers = [model.layer4]\n",
    "\n",
    "    rgb_img = cv2.imread(args.image_path, 1)[:, :, ::-1]\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "    input_tensor = preprocess_image(rgb_img,\n",
    "                                    mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "    # We have to specify the target we want to generate\n",
    "    # the Class Activation Maps for.\n",
    "    # If targets is None, the highest scoring category (for every member in the batch) will be used.\n",
    "    # You can target specific categories by\n",
    "    # targets = [e.g ClassifierOutputTarget(281)]\n",
    "    \n",
    "    targets = None\n",
    "\n",
    "    # Using the with statement ensures the context is freed, and you can\n",
    "    # recreate different CAM objects in a loop.\n",
    "\n",
    "    \n",
    "    cam = EigenCAM(model=model, )\n",
    "    cam_algorithm = methods[args.method]\n",
    "    with cam_algorithm(model=model,\n",
    "                       target_layers=target_layers,\n",
    "                       use_cuda=args.use_cuda) as cam:\n",
    "\n",
    "        # AblationCAM and ScoreCAM have batched implementations.\n",
    "        # You can override the internal batch size for faster computation.\n",
    "        cam.batch_size = 32\n",
    "        grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                            targets=targets,\n",
    "                            aug_smooth=args.aug_smooth,\n",
    "                            eigen_smooth=args.eigen_smooth)\n",
    "\n",
    "        # Here grayscale_cam has only one image in the batch\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "        cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "        # cam_image is RGB encoded whereas \"cv2.imwrite\" requires BGR encoding.\n",
    "        cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    gb_model = GuidedBackpropReLUModel(model=model, use_cuda=args.use_cuda)\n",
    "    gb = gb_model(input_tensor, target_category=None)\n",
    "\n",
    "    cam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam])\n",
    "    cam_gb = deprocess_image(cam_mask * gb)\n",
    "    gb = deprocess_image(gb)\n",
    "\n",
    "    cv2.imwrite(f'{args.method}_cam.jpg', cam_image)\n",
    "    cv2.imwrite(f'{args.method}_gb.jpg', gb)\n",
    "    cv2.imwrite(f'{args.method}_cam_gb.jpg', cam_gb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
