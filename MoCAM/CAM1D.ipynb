{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94d4c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weight: runs/train/exp130/weights/train-200.pt\n",
      "torch.Size([1, 105, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taehyun/workspace/childtoy/MotionReasoning/MoCAM/model/model.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import yaml\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_proc.emotionmocap_dataset import EmotionDataset\n",
    "from data_proc.utils import increment_path\n",
    "from model.model import TCN, PURE1D\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "from CAM.eigen_cam import EigenCAM\n",
    "\n",
    "from CAM.guided_backprop import GuidedBackpropReLUModel\n",
    "from CAM.utils.image import show_cam_on_image, deprocess_image, preprocess_image\n",
    "from CAM.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "parser = argparse.ArgumentParser()\n",
    "project='runs/train'\n",
    "weight='latest'\n",
    "exp_name='exp130'\n",
    "data_path='/home/taehyun/workspace/childtoy/MotionReasoning/dataset/mocap_emotion_rig'\n",
    "window=80\n",
    "batch_size=1\n",
    "\n",
    "processed_data_dir='processed_data_mocam/'\n",
    "\n",
    "save_dir = Path(os.path.join('runs', 'train', exp_name))\n",
    "wdir = save_dir / 'weights'\n",
    "weights = os.listdir(wdir)\n",
    "\n",
    "if weight == 'latest':\n",
    "    weights_paths = [wdir / weight for weight in weights]\n",
    "    weight_path = max(weights_paths , key = os.path.getctime)\n",
    "else:\n",
    "    weight_path = wdir / ('train-' + weight + '.pt')\n",
    "ckpt = torch.load(weight_path, map_location=device)\n",
    "print(f\"Loaded weight: {weight_path}\")\n",
    "\n",
    "\n",
    "# Load LAFAN Dataset\n",
    "Path(processed_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "emotion_dataset = EmotionDataset(data_dir=data_path, processed_data_dir=processed_data_dir, train=False, device=device, window=window)\n",
    "emotion_data_loader = DataLoader(emotion_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "n_hid = 70\n",
    "n_level = 4\n",
    "channel_sizes = [n_hid] * n_level\n",
    "kernel_size = 5\n",
    "model = PURE1D(input_channels, n_classes, kernel_size=kernel_size, dropout=0)\n",
    "model.load_state_dict(ckpt['TCN'])\n",
    "model.eval()\n",
    "correct = 0\n",
    "n_classes = ckpt['n_classes']\n",
    "input_channels = ckpt['input_channels']\n",
    "seq_length = 40\n",
    "n_hid = ckpt['n_hid']\n",
    "n_level = ckpt['n_level']\n",
    "n_classes = 7\n",
    "input_channels = 105\n",
    "origin_data = iter(emotion_data_loader).next()# confusion_matrix = torch.zeros(7, 7)\n",
    "local_q = origin_data[\"local_q\"].to(device)\n",
    "q_vel = origin_data[\"q_vel\"].to(device) \n",
    "q_acc = origin_data[\"q_acc\"].to(device) \n",
    "labels = origin_data[\"labels\"].to(device)\n",
    "data = torch.cat([local_q, q_vel, q_acc], axis=2)\n",
    "data = data.permute(0,2,1)\n",
    "output = model(data)\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d09a6c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 105, 80])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5bade33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = EigenCAM(model=model, target_layers = model.net.net3, use_cuda=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03b2aa05",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EigenCAM' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4ef783462258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'EigenCAM' object has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "with cam :\n",
    "\n",
    "    # AblationCAM and ScoreCAM have batched implementations.\n",
    "    # You can override the internal batch size for faster computation.\n",
    "    cam.batch_size = 1\n",
    "    grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                        targets=targets,\n",
    "                        aug_smooth=args.aug_smooth,\n",
    "                        eigen_smooth=args.eigen_smooth)\n",
    "\n",
    "    # Here grayscale_cam has only one image in the batch\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    # cam_image is RGB encoded whereas \"cv2.imwrite\" requires BGR encoding.\n",
    "    cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea2cd3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from CAM.utils.find_layer import replace_all_layer_type_recursive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482d97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ef4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a84ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99f38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model = models.resnet50(pretrained=True)\n",
    "target_layers = [model.layer4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd88bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32609e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--use-cuda', action='store_true', default=False,\n",
    "                        help='Use NVIDIA GPU acceleration')\n",
    "    parser.add_argument(\n",
    "        '--image-path',\n",
    "        type=str,\n",
    "        default='./examples/both.png',\n",
    "        help='Input image path')\n",
    "    parser.add_argument('--aug_smooth', action='store_true',\n",
    "                        help='Apply test time augmentation to smooth the CAM')\n",
    "    parser.add_argument(\n",
    "        '--eigen_smooth',\n",
    "        action='store_true',\n",
    "        help='Reduce noise by taking the first principle componenet'\n",
    "        'of cam_weights*activations')\n",
    "    parser.add_argument('--method', type=str, default='gradcam',\n",
    "                        choices=['gradcam', 'gradcam++',\n",
    "                                 'scorecam', 'xgradcam',\n",
    "                                 'ablationcam', 'eigencam',\n",
    "                                 'eigengradcam', 'layercam', 'fullgrad'],\n",
    "                        help='Can be gradcam/gradcam++/scorecam/xgradcam'\n",
    "                             '/ablationcam/eigencam/eigengradcam/layercam')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
    "    if args.use_cuda:\n",
    "        print('Using GPU for acceleration')\n",
    "    else:\n",
    "        print('Using CPU for computation')\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" python cam.py -image-path <path_to_image>\n",
    "    Example usage of loading an image, and computing:\n",
    "        1. CAM\n",
    "        2. Guided Back Propagation\n",
    "        3. Combining both\n",
    "    \"\"\"\n",
    "\n",
    "    args = get_args()\n",
    "    methods = \n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # Choose the target layer you want to compute the visualization for.\n",
    "    # Usually this will be the last convolutional layer in the model.\n",
    "    # Some common choices can be:\n",
    "    # Resnet18 and 50: model.layer4\n",
    "    # VGG, densenet161: model.features[-1]\n",
    "    # mnasnet1_0: model.layers[-1]\n",
    "    # You can print the model to help chose the layer\n",
    "    # You can pass a list with several target layers,\n",
    "    # in that case the CAMs will be computed per layer and then aggregated.\n",
    "    # You can also try selecting all layers of a certain type, with e.g:\n",
    "    # from pytorch_grad_cam.utils.find_layers import find_layer_types_recursive\n",
    "    # find_layer_types_recursive(model, [torch.nn.ReLU])\n",
    "    target_layers = [model.layer4]\n",
    "\n",
    "    rgb_img = cv2.imread(args.image_path, 1)[:, :, ::-1]\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "    input_tensor = preprocess_image(rgb_img,\n",
    "                                    mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "    # We have to specify the target we want to generate\n",
    "    # the Class Activation Maps for.\n",
    "    # If targets is None, the highest scoring category (for every member in the batch) will be used.\n",
    "    # You can target specific categories by\n",
    "    # targets = [e.g ClassifierOutputTarget(281)]\n",
    "    \n",
    "    targets = None\n",
    "\n",
    "    # Using the with statement ensures the context is freed, and you can\n",
    "    # recreate different CAM objects in a loop.\n",
    "\n",
    "    \n",
    "    cam = EigenCAM(model=model, )\n",
    "    cam_algorithm = methods[args.method]\n",
    "    with cam_algorithm(model=model,\n",
    "                       target_layers=target_layers,\n",
    "                       use_cuda=args.use_cuda) as cam:\n",
    "\n",
    "        # AblationCAM and ScoreCAM have batched implementations.\n",
    "        # You can override the internal batch size for faster computation.\n",
    "        cam.batch_size = 32\n",
    "        grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                            targets=targets,\n",
    "                            aug_smooth=args.aug_smooth,\n",
    "                            eigen_smooth=args.eigen_smooth)\n",
    "\n",
    "        # Here grayscale_cam has only one image in the batch\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "        cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "        # cam_image is RGB encoded whereas \"cv2.imwrite\" requires BGR encoding.\n",
    "        cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    gb_model = GuidedBackpropReLUModel(model=model, use_cuda=args.use_cuda)\n",
    "    gb = gb_model(input_tensor, target_category=None)\n",
    "\n",
    "    cam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam])\n",
    "    cam_gb = deprocess_image(cam_mask * gb)\n",
    "    gb = deprocess_image(gb)\n",
    "\n",
    "    cv2.imwrite(f'{args.method}_cam.jpg', cam_image)\n",
    "    cv2.imwrite(f'{args.method}_gb.jpg', gb)\n",
    "    cv2.imwrite(f'{args.method}_cam_gb.jpg', cam_gb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
